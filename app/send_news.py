#!/usr/bin/env python3
"""
AI News Bot for Microsoft Teams
Collects and posts AI-related news to Teams channel via webhook
"""

import os
import json
import pytz
import feedparser
import requests
import re
from datetime import datetime, timedelta
from typing import List, Dict
from dataclasses import dataclass
from dateutil import parser

# Import setup_logger from util.log
from util.log import setup_logger

# Setup logging
logger = setup_logger(__name__)

# Configuration
@dataclass
class Config:
    """Configuration settings"""
    teams_webhook_url: str = os.getenv("TEAMS_WEBHOOK_URL", "")
    llm_endpoint: str = os.getenv("LLM_ENDPOINT", "http://192.168.131.193:8008/v1/chat/completions")
    llm_model: str = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
    max_news_items: int = int(os.getenv("MAX_NEWS_ITEMS", "3"))
    dry_run: bool = os.getenv("DRY_RUN", "false").lower() == "true"

# RSS Feed Sources - Japanese AI/Tech News
RSS_FEEDS = [
    # æ—¥æœ¬ã®AIãƒ»æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ
    ("ITmedia AI+ RSS", "https://rss.itmedia.co.jp/rss/2.0/aiplus.xml"),
    ("ITmedia NEWS AI", "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml"),
    ("ITmedia ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚º", "https://rss.itmedia.co.jp/rss/2.0/enterprise.xml"),
    ("@IT", "https://rss.itmedia.co.jp/rss/2.0/ait.xml"),
    ("@IT Security", "https://rss.itmedia.co.jp/rss/2.0/ait_security.xml"),
    ("GIGAZINE", "https://gigazine.net/news/rss_2.0/"),
    ("ZDNet Japan", "https://feeds.japan.zdnet.com/rss/zdnet/all.rdf"),
    ("CNET Japan", "https://feeds.japan.cnet.com/rss/cnet/all.rdf"),
    ("Publickey", "https://www.publickey1.jp/atom.xml"),
    ("ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒãƒ¼ã‚¯ ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼", "https://b.hatena.ne.jp/hotentry/it.rss"),
    ("Developers.IO", "https://dev.classmethod.jp/feed/"),
    ("Qiita ãƒˆãƒ¬ãƒ³ãƒ‰", "https://qiita.com/popular-items/feed"),
    ("Zenn ãƒˆãƒ¬ãƒ³ãƒ‰", "https://zenn.dev/feed"),
]

class NewsCollector:
    """Collects news from various RSS feeds"""

    def __init__(self, feeds: List[tuple], hours_back: int = 3, max_entries_per_feed: int = 30):
        self.feeds = feeds
        self.hours_back = hours_back  # ä½•æ™‚é–“å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹
        self.max_entries_per_feed = max_entries_per_feed  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹æœ€å¤§è¨˜äº‹æ•°

    def fetch_articles(self) -> List[Dict]:
        """Fetch articles from all configured RSS feeds"""
        all_articles = []

        # ç¾åœ¨æ™‚åˆ»ã‹ã‚‰æŒ‡å®šæ™‚é–“å‰ã¾ã§ã‚’å–å¾—å¯¾è±¡ã¨ã™ã‚‹
        jst = pytz.timezone('Asia/Tokyo')
        cutoff_time = datetime.now(jst) - timedelta(hours=self.hours_back)

        # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã®çµ±è¨ˆã‚’ä¿å­˜
        feed_stats = {}

        for source_name, feed_url in self.feeds:
            try:
                logger.info(f"Fetching from {source_name}...")
                feed = feedparser.parse(feed_url)
                articles_added = 0

                # â‘  å„RSS_FEEDã‹ã‚‰å–å¾—ã—ãŸç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°å‡ºåŠ›
                logger.info(f"  Raw feed data from {source_name}:")
                logger.info(f"    Feed entries count: {len(feed.entries)}")
                if len(feed.entries) > 0:
                    logger.info(f"    Sample entry (first): {json.dumps(dict(feed.entries[0]), indent=2, default=str, ensure_ascii=False)[:1000]}...")

                total_in_feed = 0  # ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã—ãŸç·è¨˜äº‹æ•°
                recent_count = 0   # æŒ‡å®šæ™‚é–“å†…ã®è¨˜äº‹æ•°

                for entry in feed.entries[:self.max_entries_per_feed]:  # ç’°å¢ƒå¤‰æ•°ã§åˆ¶å¾¡
                    total_in_feed += 1
                    # å…¬é–‹æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹
                    published_str = entry.get("published", "")
                    if not published_str:
                        # å…¬é–‹æ—¥æ™‚ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                        continue

                    try:
                        published_dt = parser.parse(published_str)
                        if published_dt.tzinfo is None:
                            published_dt = jst.localize(published_dt)

                        # æŒ‡å®šæ™‚é–“ã‚ˆã‚Šå¤ã„è¨˜äº‹ã¯ã‚¹ã‚­ãƒƒãƒ—
                        if published_dt < cutoff_time:
                            continue
                    except:
                        continue  # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯å«ã‚ãªã„

                    article = {
                        "title": entry.get("title", ""),
                        "url": entry.get("link", ""),
                        "source": source_name,
                        "published": published_str,
                        "summary": entry.get("summary", "")[:500] if entry.get("summary") else ""
                    }
                    logger.info(f'{article["title"]=}, {article["url"]=}, {article["source"]=}, {article["published"]=}')
                    all_articles.append(article)
                    articles_added += 1
                    recent_count += 1

                logger.info(f"  Fetched {articles_added} articles from {source_name} (last {self.hours_back} hours)")

                feed_stats[source_name] = {
                    'total_fetched': total_in_feed,
                    'recent_count': recent_count
                }

            except Exception as e:
                logger.error(f"Error fetching from {source_name}: {e}")
                feed_stats[source_name] = {
                    'total_fetched': 0,
                    'recent_count': 0
                }

        # Log summary of articles collected per source
        logger.info("=" * 60)
        logger.info("ARTICLES COLLECTED PER SOURCE:")
        source_counts = {}
        for article in all_articles:
            source = article['source']
            source_counts[source] = source_counts.get(source, 0) + 1

        for source_name, _ in self.feeds:
            count = source_counts.get(source_name, 0)
            logger.info(f"  {source_name}: {count} articles")

        logger.info("=" * 60)
        logger.info(f"Total articles collected: {len(all_articles)}")

        # source_counts ã¨ feed_stats ã‚’ä¿å­˜ã—ã¦å¾Œã§ä½¿ç”¨
        self.source_counts = source_counts
        self.feed_stats = feed_stats

        return all_articles

class NewsFilter:
    """Filters news using LLM for relevance"""

    def __init__(self, config: Config):
        self.config = config

    def filter_articles(self, articles: List[Dict]) -> List[Dict]:
        """Filter articles using LLM for relevance"""

        if not articles:
            return []

        # Remove duplicate titles
        seen_titles = set()
        unique_articles = []
        for article in articles:
            title = article.get('title', '').strip()
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_articles.append(article)

        logger.info(f"Removed {len(articles) - len(unique_articles)} duplicate articles based on title")
        articles = unique_articles

        # LLMã«é€ã‚‹è¨˜äº‹æ•°ã®ä¸Šé™ï¼ˆç’°å¢ƒå¤‰æ•°ã§è¨­å®šå¯èƒ½ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ40ï¼‰
        max_articles_to_llm = int(os.getenv('MAX_ARTICLES_TO_LLM', '40'))

        # Prepare article list for LLM
        article_list = "\n".join([
            f"{i+1}. {art['title']} (from {art['source']})"
            for i, art in enumerate(articles[:max_articles_to_llm])  # Limit to prevent context overflow
        ])

        # â‘¡ LLMã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå…¨æ–‡ï¼‰ã‚’ãƒ­ã‚°å‡ºåŠ›
        prompt = f"""ä»¥ä¸‹ã®è¨˜äº‹ãƒªã‚¹ãƒˆã‹ã‚‰ã€AIã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆãƒ»å¼è­·å£«å‘ã‘ã«ç‰¹ã«é–¢é€£æ€§ã®é«˜ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸ã‚“ã§ãã ã•ã„ã€‚

å„ªå…ˆåº¦ã®é«˜ã„ã‚«ãƒ†ã‚´ãƒªï¼ˆå°‚é–€æ€§é‡è¦–ï¼‰:
1. ã€æœ€å„ªå…ˆã€‘æ³•å‹™ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹é–¢é€£
   - AIè¦åˆ¶ãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹ï¼ˆEU AI Actã€æ—¥æœ¬ã®AIã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³æ”¹è¨‚ãªã©ï¼‰
   - å€‹äººæƒ…å ±ä¿è­·ï¼ˆAPPIã€GDPRï¼‰ãƒ»ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼é–¢é€£ã®å‹•å‘
   - ã‚¯ãƒ©ã‚¦ãƒ‰ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£èªè¨¼ï¼ˆISO27001ã€ISMAPã€SOC2ã€FedRAMPï¼‰ã®æ›´æ–°
   - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆãƒ»è„†å¼±æ€§ï¼ˆCVEã€ãƒ‡ãƒ¼ã‚¿æ¼æ´©ã€ä¸æ­£ã‚¢ã‚¯ã‚»ã‚¹ã€ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³æ”»æ’ƒï¼‰
   - æ³•å¾‹æ¥­ç•Œã§ã®AIæ´»ç”¨äº‹ä¾‹ï¼ˆå¥‘ç´„æ›¸ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€æ³•çš„æ–‡æ›¸è‡ªå‹•ç”Ÿæˆã€åˆ¤ä¾‹æ¤œç´¢ï¼‰   

2. ã€é«˜å„ªå…ˆã€‘å®Ÿå‹™ã«ç›´çµã™ã‚‹AIãƒ„ãƒ¼ãƒ«ãƒ»æŠ€è¡“
   - æ¥­å‹™ã«ãŠã‘ã‚‹ç”ŸæˆAIæ´»ç”¨ã®å…·ä½“çš„äº‹ä¾‹
   - ãƒ­ãƒ¼ã‚«ãƒ«ç”Ÿæˆ AIï¼ˆSLM/VLM/OCR/éŸ³å£°èªè­˜ç­‰ï¼‰ã®æ–°æ©Ÿèƒ½ãƒ»ç²¾åº¦å‘ä¸Šãƒ»æ¯”è¼ƒ
   - Claude Codeã€MCPãªã©AIé–‹ç™ºæ”¯æ´ãƒ„ãƒ¼ãƒ«ã®æ›´æ–°ãƒ»Tips
   - AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè©•ä¾¡ãƒ»çµ±åˆãƒ„ãƒ¼ãƒ«ï¼ˆDifyã€n8nã€LangGraphã€CrewAIï¼‰

3. ã€é«˜å„ªå…ˆã€‘SLMï¼ˆå°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã®é€²åŒ–
   - è»½é‡ãƒ»é«˜é€Ÿãƒ»çœé›»åŠ›ãªAIãƒ¢ãƒ‡ãƒ«ï¼ˆPhiã€Gemmaã€å¯Œå£«é€šTakaneã€NEC cotomiã€Qwenç­‰ï¼‰
   - ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®AIå®Ÿè¡ŒæŠ€è¡“
   - ç‰¹å®šã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ãƒ¢ãƒ‡ãƒ«ï¼ˆæ³•å‹™ã€åŒ»ç™‚ã€é‡‘èå‘ã‘ï¼‰

4. ã€ä¸­å„ªå…ˆã€‘ã‚¤ãƒ³ãƒ•ãƒ©ãƒ»é–‹ç™ºç’°å¢ƒ
   - Dockerã€WSL2ã€Dev Containeré–¢é€£ã®é‡è¦æ›´æ–°
   - GPUæœ€é©åŒ–ãƒ»ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆAWSã€Azureã€GCPï¼‰
   - Teamsã€Slackç­‰ã®ã‚³ãƒ©ãƒœãƒ„ãƒ¼ãƒ«ï¼ˆç‰¹ã«æ³•å¾‹äº‹å‹™æ‰€ãƒ»ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯æ´»ç”¨ï¼‰
   - Git/GitHubã€ãƒ†ã‚¹ãƒˆè‡ªå‹•åŒ–ï¼ˆPlaywrightç­‰ï¼‰ã®æ–°æ©Ÿèƒ½

5. ã€å‚è€ƒã€‘æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯é–¢é€£
   - Pythonã€Flaskã€FastAPIã€Uvicornã®é‡è¦æ›´æ–°
   - Elasticsearchã€MySQLã®æ–°æ©Ÿèƒ½ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ›´æ–°
   - Nginxã€Llama.cppã®æœ€é©åŒ–æŠ€è¡“

è¨˜äº‹ãƒªã‚¹ãƒˆ:
{article_list}

é¸æŠåŸºæº–ï¼ˆé‡è¦åº¦é †ï¼‰:
1. æ³•çš„ãƒªã‚¹ã‚¯ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã¸ã®å½±éŸ¿ãŒã‚ã‚‹
2. æ³•å¾‹äº‹å‹™æ‰€ãƒ»ã‚³ãƒ³ã‚µãƒ«æ¥­å‹™ã®åŠ¹ç‡åŒ–ã«ç›´çµã™ã‚‹
3. å…·ä½“çš„ãªè£½å“åãƒ»ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ»å®Ÿè£…ä¾‹ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹
4. 1ã‹æœˆä»¥å†…ã«å°å…¥ãƒ»æ¤œè¨¼å¯èƒ½ãªå®Ÿç”¨çš„ãªå†…å®¹

æœ€å¤§{self.config.max_news_items}å€‹ã®è¨˜äº‹ã‚’é¸ã‚“ã§è¿”ã—ã¦ãã ã•ã„ã€‚"""

        # LLMã«é€ä¿¡ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨æ–‡ã‚’ãƒ­ã‚°å‡ºåŠ›
        logger.info("="*80)
        logger.info("LLM REQUEST - Full Prompt:")
        logger.info("="*80)
        logger.info(prompt)
        logger.info("="*80)

        # Define JSON schema for structured output
        json_schema = {
            "type": "object",
            "properties": {
                "selected_articles": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "number": {"type": "integer"},
                            "relevance_score": {"type": "number", "minimum": 0, "maximum": 1},
                            "category": {"type": "string"},
                            "reason": {"type": "string"}
                        },
                        "required": ["number", "relevance_score", "category", "reason"]
                    }
                }
            },
            "required": ["selected_articles"]
        }

        try:
            response = requests.post(
                self.config.llm_endpoint,
                headers={
                    "Content-Type": "application/json",
                    "Authorization": "Bearer no-key"
                },
                json={
                    "model": self.config.llm_model,
                    "messages": [
                        {
                            "role": "system",
                            "content": "You are an AI news curator specializing in development tools and AI technologies. Focus on practical, immediately useful information for SaaS development teams."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "max_tokens": 3000,
                    "response_format": {
                        "type": "json_object",
                        "schema": json_schema
                    }
                },
                timeout=30
            )

            if response.status_code == 200:
                result = response.json()

                # â‘¢ LLMã‹ã‚‰å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆå…¨æ–‡ï¼‰ã‚’ãƒ­ã‚°å‡ºåŠ›
                logger.info("="*80)
                logger.info("LLM RESPONSE - Full Response:")
                logger.info("="*80)
                logger.info(json.dumps(result, indent=2, ensure_ascii=False))
                logger.info("="*80)

                content = result['choices'][0]['message']['content']

                parsed_response = json.loads(content)
                filtered_indices = parsed_response.get('selected_articles', [])

                # Sort by relevance score
                filtered_indices.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)

                # Map back to original articles
                filtered_articles = []
                for item in filtered_indices[:self.config.max_news_items]:
                    idx = item['number'] - 1
                    if 0 <= idx < len(articles[:max_articles_to_llm]):
                        article = articles[idx].copy()
                        article['relevance_score'] = item.get('relevance_score', 0)
                        article['category'] = item.get('category', 'General')
                        article['reason'] = item.get('reason', '')
                        filtered_articles.append(article)

                logger.info(f"Filtered to {len(filtered_articles)} relevant articles")

                # â‘£ ã‚µãƒãƒªãƒ¼ï¼ˆâ‘¡ã§å®Ÿéš›ã«é€ã‚‰ã‚ŒãŸè¨˜äº‹ã®è©³ç´°ï¼‰
                logger.info("")
                logger.info("="*80)
                logger.info("FILTERING SUMMARY:")
                logger.info("="*80)
                logger.info(f"  Articles sent to LLM for evaluation: {len(articles[:max_articles_to_llm])}")
                logger.info(f"  Articles selected by LLM: {len(filtered_articles)}")
                logger.info("")
                logger.info("  Selected articles detail:")
                logger.info("  " + "-"*50)

                # é¸æŠã•ã‚ŒãŸè¨˜äº‹ã‚’ã‚½ãƒ¼ã‚¹åˆ¥ã«é›†è¨ˆ
                selected_by_source = {}
                for article in filtered_articles:
                    source = article['source']
                    if source not in selected_by_source:
                        selected_by_source[source] = []
                    selected_by_source[source].append(article)

                for i, article in enumerate(filtered_articles, 1):
                    logger.info(f"  {i}. [{article.get('category', 'N/A'):15}] Score: {article.get('relevance_score', 0):.2f}")
                    logger.info(f"     Title: {article['title'][:80]}..." if len(article['title']) > 80 else f"     Title: {article['title']}")
                    logger.info(f"     Source: {article['source']}")
                    logger.info(f"     Reason: {article.get('reason', 'N/A')[:100]}..." if len(article.get('reason', '')) > 100 else f"     Reason: {article.get('reason', 'N/A')}")
                    logger.info("")

                logger.info("  " + "-"*50)
                logger.info("  Selected articles by source:")
                for source_name, articles in selected_by_source.items():
                    logger.info(f"    {source_name}: {len(articles)} articles")
                logger.info("="*80)

                return filtered_articles

            else:
                logger.error(f"LLM Error: {response.status_code} - {response.text}")

        except Exception as e:
            logger.error(f"Error filtering articles: {e}")

        # Fallback: return top articles if LLM fails
        return articles[:self.config.max_news_items]

class TeamsPublisher:
    """Publishes news to Microsoft Teams"""

    def __init__(self, config: Config):
        self.config = config

    def format_published_date(self, published_str: str) -> str:
        """Format published date to YY/MM/DD HH:mm format"""
        if not published_str:
            return ""
        try:
            # Parse the date string
            dt = parser.parse(published_str)
            # Format to YY/MM/DD HH:mm
            return dt.strftime("%y/%m/%d %H:%M")
        except:
            # Return original if parsing fails
            return published_str

    def create_news_card(self, article: Dict, index: int) -> Dict:
        """Create an Adaptive Card for a news article"""

        # Emoji mapping for categories
        category_emoji = {
            "SLM/VLM": "ğŸ¤–",
            "AI Coding Agent": "ğŸ’»",
            "AI Agent": "ğŸ”§",
            "AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": "ğŸ”’",
            "Python": "ğŸ",
            "TypeScript": "ğŸ“˜",
            "éŸ³å£°èªè­˜": "ğŸ¤",
            "OCR": "ğŸ‘ï¸",
        }

        # Find matching emoji
        emoji = "ğŸ“°"
        for key, value in category_emoji.items():
            if key in article.get('category', ''):
                emoji = value
                break

        # Format score as stars
        score = article.get('relevance_score', 0)
        if score >= 0.9:
            stars = "â­â­â­â­â­"
        elif score >= 0.8:
            stars = "â­â­â­â­"
        elif score >= 0.7:
            stars = "â­â­â­"
        elif score >= 0.5:
            stars = "â­â­"
        else:
            stars = "â­"

        return {
            "type": "message",
            "attachments": [{
                "contentType": "application/vnd.microsoft.card.adaptive",
                "content": {
                    "type": "AdaptiveCard",
                    "version": "1.2",
                    "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                    "body": [
                        {
                            "type": "TextBlock",
                            "text": f"{emoji} AI News #{index}",
                            "size": "Medium",
                            "weight": "Bolder",
                            "color": "Accent"
                        },
                        {
                            "type": "TextBlock",
                            "text": article['title'],
                            "size": "Large",
                            "weight": "Bolder",
                            "wrap": True
                        },
                        {
                            "type": "ColumnSet",
                            "columns": [
                                {
                                    "type": "Column",
                                    "width": "auto",
                                    "items": [
                                        {
                                            "type": "TextBlock",
                                            "text": f"ğŸ“‚ {article.get('category', 'General')}",
                                            "size": "Small",
                                            "color": "Good"
                                        }
                                    ]
                                },
                                {
                                    "type": "Column",
                                    "width": "auto",
                                    "items": [
                                        {
                                            "type": "TextBlock",
                                            "text": self.format_published_date(article.get('published', '')),
                                            "size": "Small"
                                        }
                                    ]
                                }
                            ]
                        },
                        {
                            "type": "TextBlock",
                            "text": f"ğŸ’¡ {article.get('reason', 'AIã«é–¢é€£ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™')}",
                            "wrap": True,
                            "size": "Medium",
                            "spacing": "Small"
                        },
                        {
                            "type": "TextBlock",
                            "text": f"{article['source']}",
                            "size": "Small",
                            "color": "Attention",
                            "spacing": "Small"
                        }
                    ],
                    "actions": [
                        {
                            "type": "Action.OpenUrl",
                            "title": "è¨˜äº‹ã‚’èª­ã‚€ ğŸ”—",
                            "url": article['url']
                        }
                    ]
                }
            }]
        }

    def post_summary_card(self, total_articles: int, filtered_count: int) -> bool:
        """Post a summary card with today's statistics"""

        jst = pytz.timezone('Asia/Tokyo')
        now = datetime.now(jst)
        today = now.strftime("%Y-%m-%d %H:%M")

        card = {
            "type": "message",
            "attachments": [{
                "contentType": "application/vnd.microsoft.card.adaptive",
                "content": {
                    "type": "AdaptiveCard",
                    "version": "1.2",
                    "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                    "body": [
                        {
                            "type": "TextBlock",
                            "text": f"ğŸ¤– AI News Daily - {today}",
                            "size": "ExtraLarge",
                            "weight": "Bolder",
                            "color": "Accent"
                        },
                        {
                            "type": "TextBlock",
                            "text": f"æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ãŠå±Šã‘ã—ã¾ã™",
                            "size": "Medium",
                            "spacing": "Small"
                        },
                        {
                            "type": "FactSet",
                            "facts": [
                                {"title": "ğŸ“Š åé›†è¨˜äº‹æ•°", "value": str(total_articles)},
                                {"title": "âœ… é¸æŠè¨˜äº‹æ•°", "value": str(filtered_count)},
                                {"title": "ğŸ¯ å¯¾è±¡", "value": "æ³•å¾‹äº‹å‹™æ‰€å†… AI Product é–‹ç™ºãƒãƒ¼ãƒ "},
                                {"title": "ğŸ” ãƒ•ã‚©ãƒ¼ã‚«ã‚¹", "value": "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£, AI/ML, Coding Agent, LangGraphç­‰"}
                            ]
                        }
                    ]
                }
            }]
        }

        if self.config.dry_run:
            logger.info("DRY RUN - Would post summary card")
            return True

        try:
            response = requests.post(
                self.config.teams_webhook_url,
                json=card,
                timeout=10
            )

            return response.status_code in [200, 202, 1]

        except Exception as e:
            logger.error(f"Error posting summary: {e}")
            return False

    def post_article(self, article: Dict, index: int) -> bool:
        """Post a single article to Teams"""

        card = self.create_news_card(article, index)

        if self.config.dry_run:
            logger.info(f"DRY RUN - Would post: {article['title']}")
            return True

        try:
            response = requests.post(
                self.config.teams_webhook_url,
                json=card,
                timeout=10
            )

            if response.status_code in [200, 202, 1]:
                logger.info(f"Posted: {article['title']}")
                return True
            else:
                logger.error(f"Failed to post: {response.status_code}")
                return False

        except Exception as e:
            logger.error(f"Error posting article: {e}")
            return False

    def create_combined_news_card(self, articles: List[Dict], total_collected: int) -> Dict:
        """Create a single Adaptive Card containing all news articles"""

        jst = pytz.timezone('Asia/Tokyo')
        now = datetime.now(jst)
        today = now.strftime("%Y-%m-%d %H:%M")

        # Create body sections for all articles
        body_sections = [
            {
                "type": "TextBlock",
                "text": f"ğŸ¤– AI News Daily - {today}",
                "size": "ExtraLarge",
                "weight": "Bolder",
                "color": "Accent"
            },
            {
                "type": "TextBlock",
                "text": f"æœ¬æ—¥ã®AIãƒ‹ãƒ¥ãƒ¼ã‚¹ {len(articles)}ä»¶ã‚’ãŠå±Šã‘ã—ã¾ã™",
                "size": "Medium",
                "spacing": "Small"
            },
            {
                "type": "FactSet",
                "facts": [
                    {"title": "ğŸ“Š åé›†è¨˜äº‹æ•°", "value": str(total_collected)},
                    {"title": "âœ… é¸æŠè¨˜äº‹æ•°", "value": str(len(articles))},
                    {"title": "ğŸ¯ å¯¾è±¡", "value": "æ³•å¾‹äº‹å‹™æ‰€å†… AI Product é–‹ç™ºãƒãƒ¼ãƒ "}
                ]
            },
            {
                "type": "TextBlock",
                "text": "â”€" * 30,
                "separator": True,
                "spacing": "Medium"
            }
        ]

        # Add each article to the card
        for i, article in enumerate(articles, 1):
            # Emoji mapping for categories
            category_emoji = {
                "SLM/VLM": "ğŸ¤–",
                "AI Coding Agent": "ğŸ’»",
                "AI Agent": "ğŸ”§",
                "AIã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": "ğŸ”’",
                "Python": "ğŸ",
                "TypeScript": "ğŸ“˜",
                "éŸ³å£°èªè­˜": "ğŸ¤",
                "OCR": "ğŸ‘ï¸",
            }

            # Find matching emoji
            emoji = "ğŸ“°"
            for key, value in category_emoji.items():
                if key in article.get('category', ''):
                    emoji = value
                    break

            # Format score as stars
            score = article.get('relevance_score', 0)
            if score >= 0.9:
                stars = "â­â­â­â­â­"
            elif score >= 0.8:
                stars = "â­â­â­â­"
            elif score >= 0.7:
                stars = "â­â­â­"
            elif score >= 0.5:
                stars = "â­â­"
            else:
                stars = "â­"

            # Add article section
            body_sections.extend([
                {
                    "type": "TextBlock",
                    "text": f"{emoji} #{i} {article['title']}",
                    "size": "Medium",
                    "weight": "Bolder",
                    "wrap": True,
                    "color": "Accent"
                },
                {
                    "type": "ColumnSet",
                    "columns": [
                        {
                            "type": "Column",
                            "width": "auto",
                            "items": [
                                {
                                    "type": "TextBlock",
                                    "text": f"ğŸ“‚ {article.get('category', 'General')}",
                                    "size": "Small",
                                    "color": "Good"
                                }
                            ]
                        },
                        {
                            "type": "Column",
                            "width": "auto",
                            "items": [
                                {
                                    "type": "TextBlock",
                                    "text": self.format_published_date(article.get('published', '')),
                                    "size": "Small"
                                }
                            ]
                        },
                        {
                            "type": "Column",
                            "width": "stretch",
                            "items": [
                                {
                                    "type": "TextBlock",
                                    "text": f"{article['source']}",
                                    "size": "Small",
                                    "color": "Attention"
                                }
                            ]
                        }
                    ]
                }
            ])

            # Add summary if available
            summary_text = article.get('summary', '').strip()
            if summary_text:
                # Remove HTML tags
                summary_text = re.sub(r'<[^>]+>', '', summary_text)
                # Remove multiple spaces and newlines
                summary_text = re.sub(r'\s+', ' ', summary_text).strip()

                # Limit summary length and add ellipsis if truncated
                max_length = 200
                if len(summary_text) > max_length:
                    summary_text = summary_text[:max_length] + "..."

                # Only add if there's still content after cleaning
                if summary_text:
                    body_sections.append({
                        "type": "TextBlock",
                        "text": summary_text,
                        "wrap": True,
                        "size": "Small",
                        "isSubtle": True,
                        "spacing": "Small"
                    })

            # Add action button
            body_sections.append({
                "type": "ActionSet",
                "actions": [
                    {
                        "type": "Action.OpenUrl",
                        "title": f"è¨˜äº‹ã‚’èª­ã‚€ ğŸ”—",
                        "url": article['url']
                    }
                ]
            })

            # Add separator between articles (except for last one)
            if i < len(articles):
                body_sections.append({
                    "type": "TextBlock",
                    "text": "",
                    "separator": True,
                    "spacing": "Medium"
                })

        return {
            "type": "message",
            "attachments": [{
                "contentType": "application/vnd.microsoft.card.adaptive",
                "content": {
                    "type": "AdaptiveCard",
                    "version": "1.2",
                    "$schema": "http://adaptivecards.io/schemas/adaptive-card.json",
                    "body": body_sections
                }
            }]
        }

    def publish_news(self, articles: List[Dict], total_collected: int = None) -> int:
        """Publish all news articles to Teams in a single message"""

        if not articles:
            logger.warning("No articles to publish")
            return 0

        if total_collected is None:
            total_collected = len(articles) * 3  # Fallback estimate

        # Create and post combined card
        combined_card = self.create_combined_news_card(articles, total_collected)

        if self.config.dry_run:
            logger.info(f"DRY RUN - Would post combined card with {len(articles)} articles")
            return len(articles)

        try:
            response = requests.post(
                self.config.teams_webhook_url,
                json=combined_card,
                timeout=10
            )

            if response.status_code in [200, 202, 1]:
                logger.info(f"Successfully posted combined card with {len(articles)} articles")
                return len(articles)
            else:
                logger.error(f"Failed to post combined card: {response.status_code}")
                return 0

        except Exception as e:
            logger.error(f"Error posting combined card: {e}")
            return 0

def main():
    """Main execution function"""

    logger.info("=== AI News Bot Started ===")

    # Load configuration
    config = Config()

    if config.dry_run:
        logger.info("Running in DRY RUN mode - no actual posts will be made")

    # Collect news
    hours_back = int(os.getenv('HOURS_BACK', '3'))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ3æ™‚é–“å‰ã¾ã§
    max_entries_per_feed = int(os.getenv('MAX_ENTRIES_PER_FEED', '30'))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ30ä»¶
    collector = NewsCollector(RSS_FEEDS, hours_back=hours_back, max_entries_per_feed=max_entries_per_feed)
    articles = collector.fetch_articles()

    if not articles:
        logger.error("No articles collected")
        return 1

    # Filter news
    filter = NewsFilter(config)
    filtered_articles = filter.filter_articles(articles)

    if not filtered_articles:
        logger.warning("No relevant articles found after filtering")
        return 1

    # Calculate total collected articles before publishing
    total_collected = 0
    for source_name, _ in RSS_FEEDS:
        stats = collector.feed_stats.get(source_name, {'total_fetched': 0, 'recent_count': 0})
        total_collected += stats['recent_count']

    # Publish to Teams
    publisher = TeamsPublisher(config)
    published = publisher.publish_news(filtered_articles, total_collected)

    # â‘£ å®Ÿè¡Œã‚µãƒãƒªãƒ¼
    logger.info("")
    logger.info("="*80)
    logger.info("ã€å®Ÿè¡Œã‚µãƒãƒªãƒ¼ã€‘")
    logger.info("="*80)

    # åé›†ã‚µãƒãƒªãƒ¼
    logger.info("â–  è¨˜äº‹åé›†")
    logger.info(f"  å¯¾è±¡æœŸé–“: éå» {hours_back} æ™‚é–“")
    logger.info(f"  RSSãƒ•ã‚£ãƒ¼ãƒ‰æ•°: {len(RSS_FEEDS)} ä»¶")
    logger.info(f"  å„ãƒ•ã‚£ãƒ¼ãƒ‰æœ€å¤§å–å¾—æ•°: {max_entries_per_feed} ä»¶")
    logger.info("")

    # ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥è©³ç´°
    logger.info("  ãƒ•ã‚£ãƒ¼ãƒ‰åˆ¥åé›†çµæœ: (éå»{0}æ™‚é–“å†…/å–å¾—ç·æ•°)".format(hours_back))
    logger.info("  " + "-"*65)

    total_collected = 0
    total_fetched = 0
    for source_name, _ in RSS_FEEDS:
        stats = collector.feed_stats.get(source_name, {'total_fetched': 0, 'recent_count': 0})
        recent = stats['recent_count']
        fetched = stats['total_fetched']
        total_collected += recent
        total_fetched += fetched

        status = "âœ“" if recent > 0 else "âœ—"
        ratio_str = f"{recent:2}/{fetched:2}"
        logger.info(f"  {status} {source_name:40} : {ratio_str:7} ä»¶")

    logger.info("  " + "-"*65)
    logger.info(f"  åˆè¨ˆ: {total_collected}/{total_fetched} ä»¶åé›†")
    logger.info("")

    # å‡¦ç†ã‚µãƒãƒªãƒ¼
    max_articles_to_llm = int(os.getenv('MAX_ARTICLES_TO_LLM', '40'))
    logger.info("â–  è¨˜äº‹å‡¦ç†")
    logger.info(f"  åé›†è¨˜äº‹æ•°: {len(articles)} ä»¶")
    logger.info(f"  LLMã¸é€ä¿¡: {min(max_articles_to_llm, len(articles))} ä»¶")
    logger.info(f"  LLMãŒé¸æŠ: {len(filtered_articles)} ä»¶")
    logger.info(f"  TeamsæŠ•ç¨¿: {published} ä»¶")
    logger.info("")

    # å®Ÿè¡Œçµæœ
    logger.info("â–  å®Ÿè¡Œçµæœ")
    logger.info(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {'æˆåŠŸ âœ“' if published > 0 else 'å¤±æ•— âœ—'}")
    logger.info("="*80)

    logger.info(f"=== AI News Bot å®Œäº† - {published}ä»¶ã®è¨˜äº‹ã‚’æŠ•ç¨¿ã—ã¾ã—ãŸ ===")
    return 0 if published > 0 else 1

if __name__ == "__main__":
    exit(main())